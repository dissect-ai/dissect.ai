# complete_disectai_pipeline.py
# Repository-friendly pipeline: download Kaggle dataset, extract multi-part zip, build/train/evaluate model, and save.

import os
import shutil
import subprocess
from glob import glob

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import EfficientNetB0
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import pathlib
import matplotlib.pyplot as plt


# ------------------------------
# 0. Configuration
# ------------------------------
# Set your Kaggle dataset identifier here (username/dataset-name)
KAGGLE_DATASET = "xiaoweixumedicalai/imagetbad"

WORK_DIR = os.getcwd()  # current directory (GitHub repo)
DATA_DIR = os.path.join(WORK_DIR, "data")
EXTRACT_DIR = os.path.join(WORK_DIR, "imageTBAD_extracted")
MODEL_DIR = os.path.join(WORK_DIR, "model")

IMG_SIZE = (224, 224)
BATCH_SIZE = 32
EPOCHS = 15
AUTOTUNE = tf.data.AUTOTUNE

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(EXTRACT_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)


# ------------------------------
# 1. Download & unzip from Kaggle
# ------------------------------
def download_kaggle_dataset(dataset, download_path):
    """
    Uses Kaggle API to download and unzip dataset.
    Requires KAGGLE_USERNAME and KAGGLE_KEY to be set as environment variables.
    """
    print(f"Downloading Kaggle dataset {dataset} into {download_path}...")
    subprocess.run(
        ["kaggle", "datasets", "download", "-d", dataset, "-p", download_path, "--unzip"],
        check=True,
    )
    print("Download complete.")


# ------------------------------
# 2. Extract multi-part zip archive
# ------------------------------
def extract_multipart_zip(input_dir, output_dir):
    """
    Finds all parts imageTBAD.z01..zNN in input_dir, copies to working dir, 
    and uses 7z to extract into output_dir.
    """
    parts = sorted([f for f in os.listdir(input_dir) if f.startswith("imageTBAD.z")])
    if not parts:
        raise FileNotFoundError("No multi-part zip files found in data directory.")

    # Copy parts to working directory
    for part in parts:
        src = os.path.join(input_dir, part)
        dst = os.path.join(WORK_DIR, part)
        if not os.path.exists(dst):
            shutil.copy(src, dst)

    # Extract once into EXTRACT_DIR
    if not os.listdir(output_dir):
        first_part = os.path.join(WORK_DIR, parts[0])
        print(f"Extracting multipart archive from {first_part} into {output_dir} ...")
        subprocess.run(
            ["7z", "x", first_part, f"-o{output_dir}"],
            check=True,
        )
        print("Extraction complete.")
    else:
        print("Extraction directory already contains files.")


# ------------------------------
# 3. Locate Train & Val folders
# ------------------------------
def find_folder(name, root):
    """
    Recursively search for a folder named `name` (case-insensitive) under `root`.
    """
    for dirpath, dirnames, _ in os.walk(root):
        if os.path.basename(dirpath).lower() == name.lower():
            return dirpath
    return None


# ------------------------------
# 4. Build tf.data datasets
# ------------------------------
def preprocess_image(path, label):
    image = tf.io.read_file(path)
    image = tf.image.decode_png(image, channels=3)
    image = tf.image.resize(image, IMG_SIZE)
    image = image / 255.0
    return image, label


def get_dataset(directory):
    """
    Creates a tf.data.Dataset of (image_tensor, label) from a directory where
    subfolders are class names (e.g. '0' and '1').
    """
    data_root = pathlib.Path(directory)
    all_image_paths = list(data_root.glob("*/*"))
    all_image_paths = [str(path) for path in all_image_paths]
    label_names = sorted(item.name for item in data_root.glob("*/") if item.is_dir())
    label_to_index = dict((name, index) for index, name in enumerate(label_names))
    all_labels = [label_to_index[pathlib.Path(path).parent.name] for path in all_image_paths]

    path_ds = tf.data.Dataset.from_tensor_slices(all_image_paths)
    label_ds = tf.data.Dataset.from_tensor_slices(all_labels)
    ds = tf.data.Dataset.zip((path_ds, label_ds))
    ds = ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)
    return ds


# ------------------------------
# 5. Build and compile model
# ------------------------------
def build_model():
    """
    Builds an EfficientNetB0-based binary classifier with mixed precision.
    """
    tf.keras.mixed_precision.set_global_policy("mixed_float16")
    base = EfficientNetB0(include_top=False, input_shape=IMG_SIZE + (3,), weights="imagenet")
    base.trainable = False
    model = models.Sequential(
        [
            base,
            layers.GlobalAveragePooling2D(),
            layers.Dense(1, activation="sigmoid", dtype="float32"),
        ]
    )
    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
        loss="binary_crossentropy",
        metrics=["accuracy"],
    )
    return model


# ------------------------------
# 6. Training & fine-tuning
# ------------------------------
def train_and_finetune(train_ds, val_ds):
    model = build_model()
    early_stop = EarlyStopping(monitor="val_loss", patience=3, restore_best_weights=True)
    checkpoint = ModelCheckpoint(os.path.join(MODEL_DIR, "best_model.h5"), monitor="val_accuracy", save_best_only=True)

    print("Starting initial training (frozen backbone)...")
    history1 = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=EPOCHS,
        callbacks=[early_stop, checkpoint],
        verbose=2,
    )

    # Unfreeze top half of EfficientNetB0
    model.get_layer(index=0).trainable = True
    fine_tune_at = len(model.layers[0].layers) // 2
    for layer in model.layers[0].layers[:fine_tune_at]:
        layer.trainable = False

    model.compile(
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
        loss="binary_crossentropy",
        metrics=["accuracy"],
    )
    print("Starting fine-tuning (unfrozen top layers)...")
    history2 = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=EPOCHS // 2,
        callbacks=[early_stop, checkpoint],
        verbose=2,
    )

    model.load_weights(os.path.join(MODEL_DIR, "best_model.h5"))
    val_loss, val_acc = model.evaluate(val_ds, verbose=0)
    print(f"Final validation accuracy: {val_acc * 100:.2f}%")
    model.save(os.path.join(MODEL_DIR, "final_model.h5"))
    print("Final model saved to", os.path.join(MODEL_DIR, "final_model.h5"))

    return history1, history2


# ------------------------------
# 7. Plot training curves (optional)
# ------------------------------
def plot_history(hist1, hist2):
    loss = hist1.history["loss"] + hist2.history["loss"]
    val_loss = hist1.history["val_loss"] + hist2.history["val_loss"]
    acc = hist1.history["accuracy"] + hist2.history["accuracy"]
    val_acc = hist1.history["val_accuracy"] + hist2.history["val_accuracy"]

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(loss, label="train loss")
    plt.plot(val_loss, label="val loss")
    plt.legend()
    plt.title("Loss")

    plt.subplot(1, 2, 2)
    plt.plot(acc, label="train acc")
    plt.plot(val_acc, label="val acc")
    plt.legend()
    plt.title("Accuracy")
    plt.show()


# ------------------------------
# 8. Main pipeline
# ------------------------------
if __name__ == "__main__":
    # 1. Download Kaggle dataset (requires KAGGLE_USERNAME and KAGGLE_KEY set in env)
    download_kaggle_dataset(KAGGLE_DATASET, DATA_DIR)

    # 2. Extract multi-part archive
    extract_multipart_zip(DATA_DIR, EXTRACT_DIR)

    # 3. Locate Train & Val directories
    train_folder = find_folder("Train", EXTRACT_DIR)
    val_folder = find_folder("Val", EXTRACT_DIR)
    if not train_folder or not val_folder:
        raise FileNotFoundError("Train/Val folders not found after extraction.")

    print("Using train folder:", train_folder)
    print("Using val folder:  ", val_folder)

    # 4. Create tf.data datasets
    train_ds = get_dataset(train_folder)
    val_ds = get_dataset(val_folder)
    train_ds = train_ds.shuffle(1000).batch(BATCH_SIZE).prefetch(AUTOTUNE)
    val_ds = val_ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)

    # 5-6. Train and fine-tune
    h1, h2 = train_and_finetune(train_ds, val_ds)

    # 7. Plot curves if running locally (optional)
    try:
        plot_history(h1, h2)
    except Exception:
        pass  # matplotlib may not display in all CI environments
